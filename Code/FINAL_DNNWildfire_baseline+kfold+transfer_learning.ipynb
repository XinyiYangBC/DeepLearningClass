{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQmN5MdRj-vJ",
        "outputId": "a14e4321-1598-4271-f31c-29e5aa154088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.12.0)\n",
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.12/dist-packages (from xarray) (2.2.2)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.5 netCDF4-1.7.3\n",
            "using device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install xarray netCDF4 scipy torch numpy matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "import xarray as xr\n",
        "import joblib\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAlh9sjeeceM",
        "outputId": "d000a93e-5add-4f15-aa52-d091b0a58240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files in folder:\n",
            "  MC2_burnedFrac.nc4\n",
            "  JSBACH-SPITFIRE_burnedFrac.nc4\n",
            "  wildfire_surrogate4.mat\n",
            "  wildfire_surrogate6.mat\n",
            "  wildfire_surrogate9.mat\n",
            "  LPJ-GUESS-GlobFIRM_burnedFrac.nc4\n",
            "  JULES-INFERNO_burnedFrac.nc4\n",
            "  wildfire_surrogate7.mat\n",
            "  wildfire_surrogate8.mat\n",
            "  wildfire_surrogate2.mat\n",
            "  wildfire_surrogate5.mat\n",
            "  wildfire_surrogate14.mat\n",
            "  wildfire_surrogate11.mat\n",
            "  wildfire_surrogate1.mat\n",
            "  wildfire_surrogate10.mat\n",
            "  CTEM_burnedFrac.nc4\n",
            "  CLM_burnedFrac.nc4\n",
            "  LPJ-GUESS-SPITFIRE_burnedFrac.nc4\n",
            "  ORCHIDEE-SPITFIRE_burnedFrac.nc4\n",
            "  wildfire_surrogate3.mat\n",
            "  wildfire_surrogate13.mat\n",
            "  wildfire_surrogate12.mat\n",
            "  LPJ-GUESS-SIMFIRE-BLAZE_burnedFrac.nc4\n",
            "  11_17_own_data\n",
            "  11_22_transfer_learning\n",
            "  11_14_results\n",
            "  wildfire_surrogate1_DNN_softplus.pt\n",
            "  11_29_transfer_learning\n",
            "  global_wildfire_base_model.pt\n",
            "Loaded: CLM_burnedFrac.nc4\n",
            "Loaded: CTEM_burnedFrac.nc4\n",
            "Loaded: JSBACH-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: JULES-INFERNO_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-GlobFIRM_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-SIMFIRE-BLAZE_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: MC2_burnedFrac.nc4\n",
            "Loaded: ORCHIDEE-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: wildfire_surrogate1.mat\n",
            "Loaded: wildfire_surrogate10.mat\n",
            "Loaded: wildfire_surrogate11.mat\n",
            "Loaded: wildfire_surrogate12.mat\n",
            "Loaded: wildfire_surrogate13.mat\n",
            "Loaded: wildfire_surrogate14.mat\n",
            "Loaded: wildfire_surrogate2.mat\n",
            "Loaded: wildfire_surrogate3.mat\n",
            "Loaded: wildfire_surrogate4.mat\n",
            "Loaded: wildfire_surrogate5.mat\n",
            "Loaded: wildfire_surrogate6.mat\n",
            "Loaded: wildfire_surrogate7.mat\n",
            "Loaded: wildfire_surrogate8.mat\n",
            "Loaded: wildfire_surrogate9.mat\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/DNN_Wildfire/'\n",
        "mydir = data_folder\n",
        "print(\"Files in folder:\")\n",
        "for i in os.listdir(data_folder):\n",
        "  print(\" \", i)\n",
        "\n",
        "def get_path(filename):\n",
        "  return os.path.join(data_folder, filename)\n",
        "\n",
        "nc4_files = sorted([i for i in os.listdir(data_folder) if i.endswith('.nc4')])\n",
        "mat_files = sorted([i for i in os.listdir(data_folder) if i.endswith('.mat')])\n",
        "\n",
        "nc4_dataset = {}\n",
        "for file in nc4_files:\n",
        "  path = get_path(file)\n",
        "  ds = xr.open_dataset(path)\n",
        "  nc4_dataset[file] = ds\n",
        "  print(f\"Loaded: {file}\")\n",
        "\n",
        "mat_dataset = {}\n",
        "for file in mat_files:\n",
        "  path = get_path(file)\n",
        "  ds = loadmat(path)\n",
        "  mat_dataset[file] = ds\n",
        "  print(f\"Loaded: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cRCQyeRkoXI"
      },
      "outputs": [],
      "source": [
        "class ANNWildfire(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, 5)\n",
        "    self.fc2 = nn.Linear(5, 5)\n",
        "    self.fc3 = nn.Linear(5, 5)\n",
        "    self.fc4 = nn.Linear(5, 5)\n",
        "    self.fc5 = nn.Linear(5, 5)\n",
        "    self.fc6 = nn.Linear(5, 1)\n",
        "    self.softplus = nn.Softplus()\n",
        "    #nn.init.uniform_(self.fc1.weight, a=-0.05, b=0.05)\n",
        "    #if self.fc1.bias is not None:\n",
        "      #nn.init.zeros_(self.fc1.bias)\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, -0.05, 0.05)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    pred = self.softplus(self.fc1(x))\n",
        "    pred = self.softplus(self.fc2(pred))\n",
        "    pred = self.softplus(self.fc3(pred))\n",
        "    pred = self.softplus(self.fc4(pred))\n",
        "    pred = self.softplus(self.fc5(pred))\n",
        "    pred = self.softplus(self.fc6(pred))\n",
        "\n",
        "    if y is not None:\n",
        "      loss = nn.functional.mse_loss(pred, y)\n",
        "      return loss, pred\n",
        "    return pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeC9HpyUkrHY"
      },
      "outputs": [],
      "source": [
        "def train_ann(net, train_iter, lr, epochs, device, decay_steps=1000, decay_rate=0.99):\n",
        "  net = net.to(device)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "      #x_batch, y_batch = train_data[0], train_data[1]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach().cpu())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      new_lr = lr * (decay_rate ** (step/decay_steps))\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[1][0].cpu()))\n",
        "        #print('tgt:\\t {}\\n'.format(y_batch[0].cpu()))\n",
        "  return loss_list\n",
        "##\n",
        "def predict_model(net, X, device):\n",
        "  net = net.to(device)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    X = X.to(device)\n",
        "    prediction = net(X).cpu().numpy()\n",
        "  return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4e3oGAjkud2"
      },
      "outputs": [],
      "source": [
        "def train_region(region_idx, mydir, device, force_retrain=False):\n",
        "  matfiles = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}.mat')\n",
        "  print(f\"\\n{'='*60}\")\n",
        "  print(f\"Processing region {region_idx+1}\")\n",
        "  print('='*60)\n",
        "\n",
        "  tmp = loadmat(matfiles)\n",
        "  ELMX = tmp.get('ELMX')\n",
        "  ELMy = tmp.get('ELMy')\n",
        "\n",
        "  sc_X = MinMaxScaler(feature_range=(0, 1))\n",
        "  sc_y = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "  X = sc_X.fit_transform(ELMX)\n",
        "  y = sc_y.fit_transform(ELMy.reshape(-1, 1))\n",
        "\n",
        "  scaler_filename_X = os.path.join(mydir, f\"scaler_X{region_idx+1}.mat\")\n",
        "  scaler_filename_y = os.path.join(mydir, f\"scaler_y{region_idx+1}.mat\")\n",
        "  joblib.dump(sc_X, scaler_filename_X)\n",
        "  joblib.dump(sc_y, scaler_filename_y)\n",
        "\n",
        "  y_1 = np.percentile(y, 33)\n",
        "  y_2 = np.percentile(y, 66)\n",
        "  y_3 = np.max(y)\n",
        "  strata_y = np.full([len(y), 1], 0)\n",
        "  for j in range(len(y)):\n",
        "    if y[j] <= y_1:\n",
        "      strata_y[j] = 1\n",
        "    elif y[j] <= y_2:\n",
        "      strata_y[j] = 2\n",
        "    elif y[j] <= y_3:\n",
        "      strata_y[j] = 3\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=strata_y, random_state=0)\n",
        "\n",
        "  train_dataset = TensorDataset(\n",
        "      torch.tensor(X_train, dtype=torch.float32),\n",
        "      torch.tensor(y_train, dtype=torch.float32)\n",
        "  )\n",
        "  train_iter = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "  model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_ANN_softplus.pt')\n",
        "\n",
        "  net = ANNWildfire(input_dim=X_train.shape[1])\n",
        "\n",
        "  if os.path.exists(model_path) and not force_retrain:\n",
        "    print(f\"Loading model\")\n",
        "    net.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  else:\n",
        "    print(f\"Training model\")\n",
        "    loss_list = train_ann(net, train_iter, lr=0.01, epochs=30, device=device, decay_steps=1000, decay_rate=0.99)\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "  X_all = sc_X.transform(ELMX)\n",
        "  X_all_tensor = torch.tensor(X_all, dtype=torch.float32)\n",
        "  y_pred = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  ann_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 360)\n",
        "  data_y = sc_y.inverse_transform(y.reshape(-1, 1)).reshape(-1, 360)\n",
        "\n",
        "  return ann_y, data_y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sSdl-5Xkx-p"
      },
      "outputs": [],
      "source": [
        "def tune_region(region_idx, mydir, device, obs_name='ensemble'):\n",
        "  print(f\"\\n{'='*60}\")\n",
        "  print(f\"TUNING region {region_idx+1}\")\n",
        "  print(f\"{'='*60}\")\n",
        "\n",
        "  matfiles = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}.mat')\n",
        "  tmp = loadmat(matfiles)\n",
        "  OBSX = tmp.get('OBSX')\n",
        "  OBSy_gfed = tmp.get('OBSy')\n",
        "  OBSy_cci51 = tmp.get('OBSy_cci51')\n",
        "  OBSy_ccilt11 = tmp.get('OBSy_ccilt11')\n",
        "  OBSy_mcd64 = tmp.get('OBSy_mcd64')\n",
        "  OBSy_atlas = tmp.get('OBSy_atlas')\n",
        "\n",
        "  OBSy = np.mean(np.hstack((OBSy_gfed, OBSy_cci51, OBSy_ccilt11, OBSy_mcd64, OBSy_atlas)), axis=1).reshape(-1, 1)\n",
        "\n",
        "  scaler_filename_X = os.path.join(mydir, f\"scaler_X{region_idx+1}.mat\")\n",
        "  scaler_filename_y = os.path.join(mydir, f\"scaler_y{region_idx+1}.mat\")\n",
        "  sc_X = joblib.load(scaler_filename_X)\n",
        "  sc_y = joblib.load(scaler_filename_y)\n",
        "\n",
        "  X = sc_X.transform(OBSX)\n",
        "  y = sc_y.transform(OBSy.reshape(-1, 1))\n",
        "\n",
        "  y_1 = np.percentile(y, 33)\n",
        "  y_2 = np.percentile(y, 66)\n",
        "  y_3 = np.max(y)\n",
        "  strata_y = np.full([len(y), 1], 0)\n",
        "  for j in range(len(y)):\n",
        "    if y[j] <= y_1:\n",
        "      strata_y[j] = 1\n",
        "    elif y[j] <= y_2:\n",
        "      strata_y[j] = 2\n",
        "    elif y[j] <= y_3:\n",
        "      strata_y[j] = 3\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=strata_y, random_state=0)\n",
        "  train_dataset = TensorDataset(\n",
        "      torch.tensor(X_train, dtype=torch.float32),\n",
        "      torch.tensor(y_train, dtype=torch.float32)\n",
        "  )\n",
        "  train_iter = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "  base_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_ANN_softplus.pt')\n",
        "  net = ANNWildfire(input_dim=X_train.shape[1])\n",
        "  print(f\"Loading base model: {base_model_path}\")\n",
        "  net.load_state_dict(torch.load(base_model_path, map_location=device))\n",
        "\n",
        "  tuned_model_path = ''\n",
        "  lr = 0.001\n",
        "  decay_steps = 1000\n",
        "  decay_rate = 0.9\n",
        "\n",
        "  if region_idx == 4 or region_idx == 8:\n",
        "    lr = 0.005\n",
        "    decay_steps = 3000\n",
        "    decay_rate = 0.99\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_ANN_softplus_{obs_name}_tuned4.pt')\n",
        "  elif region_idx == 7:\n",
        "    lr = 0.005\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.99\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_ANN_softplus_{obs_name}_tuned4.pt')\n",
        "  else:\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_ANN_softplus_{obs_name}_tuned2.pt')\n",
        "\n",
        "  print(f\"Tuning model with learning rate = {lr}. Saving to: {tuned_model_path}\")\n",
        "  loss_list = train_ann(net, train_iter, lr=lr, epochs=100, device=device, decay_steps=decay_steps, decay_rate=decay_rate)\n",
        "  torch.save(net.state_dict(), tuned_model_path)\n",
        "\n",
        "  #create tune_ensemble.csv files\n",
        "  X_all_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "  y_pred = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  ann_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 120)\n",
        "  data_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 120)\n",
        "\n",
        "  return np.sum(ann_y, 0), np.sum(data_y, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRZ9FoxSncEo"
      },
      "outputs": [],
      "source": [
        "class DNNWildfire(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, 5)\n",
        "    self.fc2 = nn.Linear(5, 5)\n",
        "    self.fc3 = nn.Linear(5, 5)\n",
        "    self.fc4 = nn.Linear(5, 5)\n",
        "    self.fc5 = nn.Linear(5, 5)\n",
        "    self.fc6 = nn.Linear(5, 1)\n",
        "    self.softplus = nn.Softplus()\n",
        "    #nn.init.uniform_(self.fc1.weight, a=-0.05, b=0.05)\n",
        "    #if self.fc1.bias is not None:\n",
        "      #nn.init.zeros_(self.fc1.bias)\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, -0.05, 0.05)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    pred = self.softplus(self.fc1(x))\n",
        "    pred = self.softplus(self.fc2(pred))\n",
        "    pred = self.softplus(self.fc3(pred))\n",
        "    pred = self.softplus(self.fc4(pred))\n",
        "    pred = self.softplus(self.fc5(pred))\n",
        "    #pred = self.softplus(self.fc6(pred))\n",
        "    pred = self.fc6(pred)\n",
        "\n",
        "\n",
        "    if y is not None:\n",
        "      loss = nn.functional.mse_loss(pred, y)\n",
        "      return loss, pred\n",
        "    return pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_epoch(net, val_iter, device):\n",
        "  net.eval()\n",
        "  total_loss = 0.0\n",
        "  count = 0\n",
        "  with torch.no_grad():\n",
        "    for val_data in val_iter:\n",
        "      val_data = [ds.to(device) for ds in val_data]\n",
        "      loss, pred = net(*val_data)\n",
        "      total_loss += loss.mean().item()\n",
        "      count += 1\n",
        "  net.train()\n",
        "  return total_loss / count"
      ],
      "metadata": {
        "id": "dckswdURiZyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz_1GGhdDZvM"
      },
      "outputs": [],
      "source": [
        "def train_dnn(net, train_iter, val_iter, lr, epochs, device, decay_steps=1000, decay_rate=0.99):\n",
        "  net = net.to(device)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  train_loss_per_epoch = []\n",
        "  val_loss_per_epoch = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    total_train_loss_sum = 0.0\n",
        "    batch_count = 0\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "      #x_batch, y_batch = train_data[0], train_data[1]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      total_train_loss_sum += loss.mean().item()\n",
        "      #loss_list.append(loss.mean().detach().cpu())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      new_lr = lr * (decay_rate ** (step/decay_steps))\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[1][0].cpu()))\n",
        "        #print('tgt:\\t {}\\n'.format(y_batch[0].cpu()))\n",
        "\n",
        "    #avg_train_loss = total_train_loss_sum/batch_count\n",
        "    #train_loss_per_epoch.append(avg_train_loss)\n",
        "\n",
        "    if batch_count > 0:\n",
        "        avg_train_loss = total_train_loss_sum / batch_count\n",
        "    else:\n",
        "        avg_train_loss = 0.0\n",
        "        print(f\"WARNING: Fold {e+1} had an empty training batch (batch_count=0).\")\n",
        "\n",
        "    train_loss_per_epoch.append(avg_train_loss)\n",
        "\n",
        "    val_loss = validate_epoch(net, val_iter, device)\n",
        "    val_loss_per_epoch.append(val_loss)\n",
        "  return train_loss_per_epoch, val_loss_per_epoch\n",
        "##\n",
        "def predict_model(net, X, device):\n",
        "  net = net.to(device)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    X = X.to(device)\n",
        "    prediction = net(X).cpu().numpy()\n",
        "  return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_from_own_dataset(csv_path, mydir, device, n_splits=10, epochs=100, lr=0.005, batch_size=8, random_state=0, use_log_transform=True):\n",
        "    print(f\"\\nLoad data from: {csv_path}\")\n",
        "    tmp = pd.read_csv(csv_path)\n",
        "\n",
        "    target_name = 'Burned_Area'\n",
        "    non_feature_cols = ['Date', target_name]\n",
        "    feature_names = [c for c in tmp.columns if c not in non_feature_cols]\n",
        "\n",
        "    print(f\"Found {len(feature_names)} features: \")\n",
        "    print(feature_names)\n",
        "\n",
        "    tmp = tmp.dropna(subset=feature_names + [target_name]).reset_index(drop=True)\n",
        "\n",
        "    all_dates = pd.to_datetime(tmp['Date'].values)\n",
        "\n",
        "    X = tmp[feature_names].values.astype(float)\n",
        "    y = tmp[target_name].values.astype(float).reshape(-1, 1)\n",
        "\n",
        "    if use_log_transform:\n",
        "        print(\"\\nApply log1p transform to target\")\n",
        "        y_trans = np.log1p(y)  # log(1 + y)\n",
        "    else:\n",
        "        print(\"\\nNo transform applied to target\")\n",
        "        y_trans = y.copy()\n",
        "\n",
        "    ##\n",
        "    sc_X = StandardScaler().fit(X)\n",
        "    sc_y = StandardScaler().fit(y_trans)\n",
        "\n",
        "    X_scaled = sc_X.fit_transform(X)\n",
        "    y_scaled = sc_y.fit_transform(y_trans)\n",
        "\n",
        "    #joblib.dump(sc_X, os.path.join(mydir, \"scaler_X_own_dataset.pkl\"))\n",
        "    #joblib.dump(sc_y, os.path.join(mydir, \"scaler_y_own_dataset.pkl\"))\n",
        "    #joblib.dump({'transform_method': transform_method}, os.path.join(mydir, \"preprocessing.pkl\"))\n",
        "\n",
        "    y_1 = np.percentile(y, 33)\n",
        "    y_2 = np.percentile(y, 66)\n",
        "    y_3 = np.max(y_scaled)\n",
        "    strata_y = np.full([len(y_scaled), 1], 0)\n",
        "    for j in range(len(y_scaled)):\n",
        "      if y_scaled[j] <= y_1:\n",
        "        strata_y[j] = 1\n",
        "      elif y_scaled[j] <= y_2:\n",
        "        strata_y[j] = 2\n",
        "      elif y_scaled[j] <= y_3:\n",
        "        strata_y[j] = 3\n",
        "\n",
        "    X_kfold, X_test, y_kfold, y_test, dates_kfold, dates_test, strata_kfold, strata_test = train_test_split(\n",
        "        X_scaled, y_scaled, all_dates, strata_y, test_size=0.20, stratify=strata_y, random_state=random_state\n",
        "    )\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    fold_metrics = []\n",
        "    all_preds_val = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_kfold, strata_kfold), start=1):\n",
        "      print(f\"\\nFold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}\")\n",
        "      X_train, X_val = X_kfold[train_idx], X_kfold[val_idx]\n",
        "      y_train, y_val = y_kfold[train_idx], y_kfold[val_idx]\n",
        "      dates_train, dates_val = dates_kfold[train_idx], dates_kfold[val_idx]\n",
        "\n",
        "      if len(X_train) < 2:\n",
        "        print(f\"Skipping fold {fold} due to training set being too small\")\n",
        "        continue\n",
        "\n",
        "      batch_size_train = min(batch_size, max(1, len(X_train)))\n",
        "      batch_size_val = min(batch_size, len(X_val))\n",
        "      train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.float32)\n",
        "      )\n",
        "      val_dataset = TensorDataset(\n",
        "        torch.tensor(X_val, dtype=torch.float32),\n",
        "        torch.tensor(y_val, dtype=torch.float32)\n",
        "      )\n",
        "      train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "      val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
        "      input_dim = X.shape[1]\n",
        "\n",
        "      net = build_tl_model(\n",
        "        pretrained_path=\"/path/to/pretrained.pt\",\n",
        "        input_dim=input_dim,\n",
        "        device=device,\n",
        "        freeze_until=\"fc5\"\n",
        "      )\n",
        "\n",
        "      print(f\"DEBUG: Calculated input_dim: {input_dim}\")\n",
        "      optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "      train_losses = []\n",
        "      val_losses = []\n",
        "      best_val_loss = np.inf\n",
        "      best_state = None\n",
        "      for epoch in range(1, epochs+1):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        batch_count = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, pred = net(xb, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        epoch_train_loss = running_loss / batch_count if batch_count > 0 else np.nan\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # validation\n",
        "        net.eval()\n",
        "        val_running = 0.0\n",
        "        val_count = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                loss_val, pred_val = net(xb, yb)\n",
        "                val_running += loss_val.item()\n",
        "                val_count += 1\n",
        "        epoch_val_loss = val_running / val_count if val_count > 0 else np.nan\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_state = net.state_dict()\n",
        "\n",
        "      # restore best model\n",
        "      if best_state is not None:\n",
        "        net.load_state_dict(best_state)\n",
        "      # save fold model\n",
        "      fold_model_path = os.path.join(mydir, f'own_dataset_kfold_f{fold}_model.pt')\n",
        "      torch.save(net.state_dict(), fold_model_path)\n",
        "      print(\"Saved fold model to: \", fold_model_path)\n",
        "\n",
        "      net.eval()\n",
        "      X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "      with torch.no_grad():\n",
        "        preds_val = net(X_val_tensor).cpu().numpy().flatten()\n",
        "      all_preds_val.append(preds_val)\n",
        "\n",
        "      y_val_transformed = sc_y.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
        "      preds_val_transformed = sc_y.inverse_transform(preds_val.reshape(-1, 1)).flatten()\n",
        "      if use_log_transform:\n",
        "        y_val_actual = np.expm1(y_val_transformed)\n",
        "        preds_val_actual = np.expm1(preds_val_transformed)\n",
        "      else:\n",
        "        y_val_actual = y_val_transformed\n",
        "        preds_val_actual = preds_val_transformed\n",
        "\n",
        "      r2 = r2_score(y_val_actual, preds_val_actual)\n",
        "      mse = mean_squared_error(y_val_actual, preds_val_actual)\n",
        "      fold_metrics.append({'fold': fold, 'r2': r2, 'mse': mse, 'train_losses': train_losses, 'val_losses': val_losses})\n",
        "    print(\"\\nK-Fold cross validation completed\")\n",
        "\n",
        "    # k-fold loss plot\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for m in fold_metrics:\n",
        "      fold = m['fold']\n",
        "      r2 = m['r2']\n",
        "      trainloss = m['train_losses']\n",
        "      valloss = m['val_losses']\n",
        "      epochs = np.arange(1, len(trainloss)+1)\n",
        "      plt.plot(epochs, trainloss, label=f\"Fold {fold} - Train (R² = {r2:.3f})\", alpha=0.6)\n",
        "      plt.plot(epochs, valloss, label=f\"Fold {fold} - Validation (R² = {r2:.3f})\", alpha=0.6, linestyle='--')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"K-Fold Loss Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    kfold_path = os.path.join(mydir, 'k_fold_loss_1203.png')\n",
        "    plt.savefig(kfold_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"K-Fold loss plot saved to: \", kfold_path)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    all_fold_predictions = []\n",
        "\n",
        "    print(f\"Running Ensemble Prediction on {len(X_test)} Test Samples\")\n",
        "    for fold in range(1, n_splits + 1):\n",
        "        net_fold = DNNWildfire(input_dim=X.shape[1]).to(device)\n",
        "        model_path = os.path.join(mydir, f'own_dataset_kfold_f{fold}_model.pt')\n",
        "        net_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "        net_fold.eval()\n",
        "        with torch.no_grad():\n",
        "          preds_fold = net_fold(X_test_tensor).cpu().numpy().flatten()\n",
        "          all_fold_predictions.append(preds_fold)\n",
        "\n",
        "    avg_preds_kfold = np.mean(all_fold_predictions, axis=0)\n",
        "    y_pred_transformed = sc_y.inverse_transform(avg_preds_kfold.reshape(-1, 1)).flatten()\n",
        "    y_test_transformed = sc_y.inverse_transform(y_test).flatten()\n",
        "\n",
        "    if use_log_transform:\n",
        "      y_pred_actual = np.expm1(y_pred_transformed)\n",
        "      y_test_actual = np.expm1(y_test_transformed)\n",
        "    else:\n",
        "      y_pred_actual = y_pred_transformed\n",
        "      y_test_actual = y_test_transformed\n",
        "\n",
        "    final_r2 = r2_score(y_test_actual, y_pred_actual)\n",
        "\n",
        "    print(f\"\\nFINAL ENSEMBLE PERFORMANCE (on held-out Test Set):\")\n",
        "    print(f\"R-squared (R²) Score: {final_r2:.3f}\")\n",
        "\n",
        "    # line plot actual vs pred\n",
        "    plot_tmp = pd.DataFrame({\n",
        "        'Date': dates_test,\n",
        "        'Actual': y_test_actual,\n",
        "        'Predicted': y_pred_actual\n",
        "    })\n",
        "    plot_tmp = plot_tmp.sort_values(by='Date')\n",
        "    r2 = r2_score(plot_tmp['Actual'], plot_tmp['Predicted'])\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(plot_tmp['Date'], plot_tmp['Actual'], label=\"Actual Burned Area\", color='red', linewidth=2)\n",
        "    plt.plot(plot_tmp['Date'], plot_tmp['Predicted'], label=\"Predicted Burned Area\", color='blue', linestyle='--', linewidth=1.5)\n",
        "    plt.xlabel(\"Date\", fontsize=14, fontweight='bold')\n",
        "    plt.ylabel(f\"Burned Area (Mha)\", fontsize=14, fontweight='bold')\n",
        "    plt.title(f\"Model Time-Series Comparison on Full Dataset (R²: {r2:.3f})\", fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    line_path = os.path.join(mydir, 'kfold_line_plot_actual_vs_pred_1203.png')\n",
        "    plt.savefig(line_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Line plot saved to: \", line_path)\n",
        "\n",
        "    # time-series prediction visualization plot\n",
        "    N = len(y_test_actual)\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(range(N), y_test_actual, label='Actual Burned Area', color='blue', linewidth=2)\n",
        "    plt.plot(range(N), y_pred_actual, label='Predicted Burned Area', color='yellow', linewidth=2)\n",
        "    plt.title(f\"Time-Series Prediction Visualization (R² = {r2:.3f})\")\n",
        "    plt.legend()\n",
        "    visualization_path = os.path.join(mydir, 'kfold_time-series_prediction_visualization_1203.png')\n",
        "    plt.savefig(visualization_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Time-series prediction visualization saved to: \", visualization_path)\n",
        "\n",
        "\n",
        "    return net, {'folds': fold_metrics, 'scalers': (sc_X, sc_y)}\n"
      ],
      "metadata": {
        "id": "zTKsg8Or7GcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tl_model(pretrained_path, input_dim, device, freeze_until='fc5'):\n",
        "    model = DNNWildfire(input_dim).to(device)\n",
        "\n",
        "    if pretrained_path is None or not os.path.exists(pretrained_path):\n",
        "        print(f\"[TL] No pretrained model found at {pretrained_path}. \")\n",
        "        return model\n",
        "\n",
        "    state = torch.load(pretrained_path, map_location=device)\n",
        "    if isinstance(state, dict) and 'state_dict' in state and not any(k.startswith('fc') for k in state.keys()):\n",
        "        state = state['state_dict']\n",
        "\n",
        "    model_state = model.state_dict()\n",
        "    keys_copied = []\n",
        "    for k, v in list(state.items()):\n",
        "        if k in model_state and model_state[k].shape == v.shape:\n",
        "            model_state[k] = v.clone()\n",
        "            keys_copied.append(k)\n",
        "\n",
        "    model.load_state_dict(model_state)\n",
        "    missing = [k for k in model_state.keys() if k not in keys_copied]\n",
        "    print(f\"[TL] Loaded pretrained keys: {len(keys_copied)}. Skipped/mismatched keys: {len(missing)}.\")\n",
        "\n",
        "    order = ['fc1','fc2','fc3','fc4','fc5']\n",
        "    if freeze_until is None:\n",
        "        for name, param in model.named_parameters():\n",
        "          param.requires_grad = name.startswith('fc6')\n",
        "    else:\n",
        "        freeze_flag = True\n",
        "        for lname in order:\n",
        "            layer = getattr(model, lname)\n",
        "            for p in layer.parameters():\n",
        "                p.requires_grad = freeze_flag\n",
        "            if lname == freeze_until:\n",
        "                freeze_flag = False\n",
        "        for p in model.fc6.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "def plot_kfold_histories(train_histories, val_histories, fold_results, out_path_loss):\n",
        "    plt.figure(figsize=(9,6))\n",
        "    K = len(train_histories)\n",
        "    for i in range(K):\n",
        "        tr = train_histories[i]\n",
        "        va = val_histories[i]\n",
        "        if tr is None or len(tr) == 0:\n",
        "            continue\n",
        "        r2 = fold_results[i].get('r2') if i < len(fold_results) else None\n",
        "        r2_text = f\" (R²={r2:.3f})\" if r2 is not None else \"\"\n",
        "        epochs = np.arange(1, len(tr)+1)\n",
        "        plt.plot(epochs, tr, label=f\"Fold {i+1} Train{r2_text}\", alpha=0.7)\n",
        "        plt.plot(epochs, va, linestyle='--', label=f\"Fold {i+1} Val{r2_text}\", alpha=0.6)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss (MSE scaled space)\")\n",
        "    plt.title(\"K-Fold Train / Validation Loss\")\n",
        "    plt.grid(alpha=0.2)\n",
        "    plt.legend(fontsize=9, loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path_loss, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"Saved K-fold loss plot: {out_path_loss}\")\n",
        "\n",
        "\n",
        "def train_kfold_with_tl(csv_path, mydir, device,\n",
        "                        pretrained_path=None,\n",
        "                        freeze_until='fc5',\n",
        "                        n_splits=5, epochs=60, lr=1e-3, batch_size=8,\n",
        "                        use_log_transform=True, random_state=0,\n",
        "                        out_dir=None):\n",
        "    if out_dir is None:\n",
        "        out_dir = mydir\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    target_name = 'Burned_Area'\n",
        "    feature_names = [c for c in df.columns if c not in ['Date', target_name]]\n",
        "    df = df.dropna(subset=feature_names + [target_name]).reset_index(drop=True)\n",
        "\n",
        "    X_raw = df[feature_names].values.astype(float)\n",
        "    y_raw = df[target_name].values.astype(float).reshape(-1,1)\n",
        "    dates = pd.to_datetime(df['Date'].values)\n",
        "\n",
        "    if use_log_transform:\n",
        "        y_trans = np.log1p(y_raw)\n",
        "    else:\n",
        "        y_trans = y_raw.copy()\n",
        "\n",
        "    sc_X = StandardScaler().fit(X_raw)\n",
        "    sc_y = StandardScaler().fit(y_trans)\n",
        "    X_scaled = sc_X.transform(X_raw)\n",
        "    y_scaled = sc_y.transform(y_trans)\n",
        "\n",
        "    joblib.dump(sc_X, os.path.join(out_dir, \"scaler_X_mydata.pkl\"))\n",
        "    joblib.dump(sc_y, os.path.join(out_dir, \"scaler_y_mydata.pkl\"))\n",
        "\n",
        "    y_1 = np.percentile(y_trans, 33)\n",
        "    y_2 = np.percentile(y_trans, 66)\n",
        "    y_3 = np.max(y_trans)\n",
        "    strata = np.full([len(y_trans), 1], 0)\n",
        "    for j in range(len(y_trans)):\n",
        "      if y_trans[j] <= y_1:\n",
        "        strata[j] = 1\n",
        "      elif y_trans[j] <= y_2:\n",
        "        strata[j] = 2\n",
        "      elif y_trans[j] <= y_3:\n",
        "        strata[j] = 3\n",
        "\n",
        "    X_kf_pool, X_test, y_kf_pool, y_test, dates_kf_pool, dates_test, strata_kf_pool, strata_test = \\\n",
        "        train_test_split(X_scaled, y_scaled, dates, strata, test_size=0.20, stratify=strata, random_state=random_state)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    fold_results = []\n",
        "    fold_train_histories = []\n",
        "    fold_val_histories = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_kf_pool, strata_kf_pool), start=1):\n",
        "        print(f\"\\n--- Fold {fold_idx}/{n_splits} ---\")\n",
        "        X_train, X_val = X_kf_pool[train_idx], X_kf_pool[val_idx]\n",
        "        y_train, y_val = y_kf_pool[train_idx], y_kf_pool[val_idx]\n",
        "        dates_train, dates_val = dates_kf_pool[train_idx], dates_kf_pool[val_idx]\n",
        "\n",
        "        net = DNNWildfire(input_dim=X_train.shape[1]).to(device)\n",
        "\n",
        "        if pretrained_path and os.path.exists(pretrained_path):\n",
        "          pretrained_dict = torch.load(pretrained_path, map_location=device)\n",
        "          model_dict = net.state_dict()\n",
        "\n",
        "          valid_weights = {k: v for k, v in pretrained_dict.items()\n",
        "                                 if k in model_dict and v.shape == model_dict[k].shape}\n",
        "\n",
        "          model_dict.update(valid_weights)\n",
        "          net.load_state_dict(model_dict)\n",
        "\n",
        "          if freeze_until:\n",
        "            for name, param in net.named_parameters():\n",
        "              if 'fc6' in name or 'fc1' in name:\n",
        "                param.requires_grad = True\n",
        "              elif freeze_until in name or name < freeze_until:\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "        bs_train = min(batch_size, max(1, len(X_train)))\n",
        "        bs_val = min(batch_size, max(1, len(X_val)))\n",
        "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train,dtype=torch.float32),\n",
        "                                               torch.tensor(y_train,dtype=torch.float32)),\n",
        "                                  batch_size=bs_train, shuffle=True)\n",
        "        val_loader = DataLoader(TensorDataset(torch.tensor(X_val,dtype=torch.float32),\n",
        "                                             torch.tensor(y_val,dtype=torch.float32)),\n",
        "                                batch_size=bs_val, shuffle=False)\n",
        "\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
        "\n",
        "        best_state = None\n",
        "        best_val = np.inf\n",
        "        train_hist = []\n",
        "        val_hist = []\n",
        "\n",
        "        for ep in range(1, epochs+1):\n",
        "            net.train()\n",
        "            running_loss, batch = 0.0, 0\n",
        "            for xb, yb in train_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                loss, _ = net(xb, yb)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                batch += 1\n",
        "            epoch_train_loss = running_loss / batch if batch > 0 else np.nan\n",
        "            train_hist.append(epoch_train_loss)\n",
        "\n",
        "            net.eval()\n",
        "            val_running, val_batch = 0.0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb, yb = xb.to(device), yb.to(device)\n",
        "                    vloss, _ = net(xb, yb)\n",
        "                    val_running += vloss.item(); val_batch += 1\n",
        "            epoch_val_loss = val_running / val_batch if val_batch > 0 else np.nan\n",
        "            val_hist.append(epoch_val_loss)\n",
        "\n",
        "            if epoch_val_loss < best_val:\n",
        "                best_val = epoch_val_loss\n",
        "                best_state = {k:v.cpu() for k,v in net.state_dict().items()}\n",
        "\n",
        "        if best_state is not None:\n",
        "            net.load_state_dict(best_state)\n",
        "        fold_model_path = os.path.join(out_dir, f\"kfold_fold{fold_idx}.pt\")\n",
        "        torch.save(net.state_dict(), fold_model_path)\n",
        "        print(f\"Saved fold model: {fold_model_path}\")\n",
        "\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            preds_val_scaled = net(torch.tensor(X_val, dtype=torch.float32).to(device))\n",
        "            if isinstance(preds_val_scaled, tuple):\n",
        "                _, preds_val_scaled = preds_val_scaled\n",
        "            preds_val_scaled = preds_val_scaled.cpu().numpy().flatten()\n",
        "\n",
        "        preds_val_back = sc_y.inverse_transform(preds_val_scaled.reshape(-1,1)).flatten()\n",
        "        y_val_back = sc_y.inverse_transform(y_val.reshape(-1,1)).flatten()\n",
        "        if use_log_transform:\n",
        "            preds_val_actual = np.expm1(preds_val_back)\n",
        "            y_val_actual = np.expm1(y_val_back)\n",
        "        else:\n",
        "            preds_val_actual = preds_val_back\n",
        "            y_val_actual = y_val_back\n",
        "\n",
        "        r2 = r2_score(y_val_actual, preds_val_actual)\n",
        "        mse = mean_squared_error(y_val_actual, preds_val_actual)\n",
        "        print(f\"Fold {fold_idx}: val R2={r2:.4f}, mse={mse:.4f}\")\n",
        "\n",
        "        fold_results.append({'fold': fold_idx, 'r2': r2, 'mse': mse})\n",
        "        fold_train_histories.append(train_hist)\n",
        "        fold_val_histories.append(val_hist)\n",
        "\n",
        "    out_loss = os.path.join(out_dir, \"transfer_learning_kfold_loss.png\")\n",
        "    plot_kfold_histories(fold_train_histories, fold_val_histories, fold_results, out_loss)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    all_fold_preds_test = []\n",
        "    for fold_idx in range(1, len(fold_train_histories)+1):\n",
        "        model_path = os.path.join(out_dir, f\"kfold_fold{fold_idx}.pt\")\n",
        "        if not os.path.exists(model_path):\n",
        "            continue\n",
        "        m = DNNWildfire(input_dim=X_test.shape[1]).to(device)\n",
        "        st = torch.load(model_path, map_location=device)\n",
        "\n",
        "        ms = m.state_dict()\n",
        "        for k,v in list(st.items()):\n",
        "            if k in ms and ms[k].shape == v.shape:\n",
        "                ms[k] = v\n",
        "        m.load_state_dict(ms)\n",
        "        m.eval()\n",
        "        with torch.no_grad():\n",
        "            pr = m(X_test_tensor)\n",
        "            if isinstance(pr, tuple):\n",
        "                _, pr = pr\n",
        "            all_fold_preds_test.append(pr.cpu().numpy().flatten())\n",
        "\n",
        "    avg_preds = np.mean(np.stack(all_fold_preds_test, axis=1), axis=1)\n",
        "    preds_back = sc_y.inverse_transform(avg_preds.reshape(-1,1)).flatten()\n",
        "    y_test_back = sc_y.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "    if use_log_transform:\n",
        "        preds_actual = np.expm1(preds_back)\n",
        "        y_test_actual = np.expm1(y_test_back)\n",
        "    else:\n",
        "        preds_actual = preds_back\n",
        "        y_test_actual = y_test_back\n",
        "\n",
        "    final_r2 = r2_score(y_test_actual, preds_actual)\n",
        "    final_mse = mean_squared_error(y_test_actual, preds_actual)\n",
        "    print(f\"\\nFINAL ENSEMBLE PERFORMANCE (on held-out Test set): R2={final_r2:.4f}, MSE={final_mse:.4f}\")\n",
        "\n",
        "    # line plot actual vs pred\n",
        "    df_line = pd.DataFrame({'Date': dates_test, 'Actual': y_test_actual, 'Predicted': preds_actual})\n",
        "    df_line = df_line.sort_values('Date')\n",
        "    out_line = os.path.join(out_dir, \"transfer_learning_kfold_line_plot.png\")\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(df_line['Date'], df_line['Actual'], label='Actual', color='red', linewidth=2)\n",
        "    plt.plot(df_line['Date'], df_line['Predicted'], label='Predicted', color='blue', linestyle='--')\n",
        "    plt.title(f\"K-Fold Ensemble: Actual vs Pred (R²={final_r2:.3f})\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Burned area\")\n",
        "    plt.legend(); plt.grid(alpha=0.2)\n",
        "    plt.tight_layout(); plt.savefig(out_line, dpi=200); plt.close()\n",
        "    print(f\"Saved line-plot: {out_line}\")\n",
        "\n",
        "    # time-series visualization\n",
        "    out_vis = os.path.join(out_dir, \"transfer_learning_kfold_time-series_prediction_visualization.png\")\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.plot(range(len(y_test_actual)), y_test_actual, label='Actual', linewidth=2)\n",
        "    plt.plot(range(len(preds_actual)), preds_actual, label='Predicted', linewidth=2)\n",
        "    plt.title(f\"Time-series Test Predictions (R²={final_r2:.3f})\")\n",
        "    plt.legend(); plt.grid(alpha=0.2)\n",
        "    plt.tight_layout(); plt.savefig(out_vis, dpi=200); plt.close()\n",
        "    print(f\"Saved time-series visualization: {out_vis}\")\n",
        "\n",
        "    return {\n",
        "        'fold_results': fold_results,\n",
        "        'fold_train_histories': fold_train_histories,\n",
        "        'fold_val_histories': fold_val_histories,\n",
        "        'scalers': (sc_X, sc_y),\n",
        "        'final_test': {'r2': final_r2, 'mse': final_mse},\n",
        "        'out_dir': out_dir\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xWW7r27P_vMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "csv_path = \"/content/drive/MyDrive/DNN_Wildfire/11_17_own_data/DeepLearning_Climate_Biomass_Human_Fire_dataset_2001_2024_montly_dataset.csv\"\n",
        "mydir = \"/content/drive/MyDrive/DNN_Wildfire/11_14_results\"\n",
        "outdir = \"/content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning\"\n",
        "\n",
        "pretrained_model_path = os.path.join(mydir, \"wildfire_surrogate6_ANN_softplus.pt\")\n",
        "\n",
        "results = train_kfold_with_tl(\n",
        "    csv_path=csv_path,\n",
        "    mydir=mydir,\n",
        "    device=device,\n",
        "    pretrained_path=pretrained_model_path,\n",
        "    freeze_until='fc3',\n",
        "    n_splits=10,\n",
        "    epochs=100,\n",
        "    lr=1e-3,\n",
        "    batch_size=8,\n",
        "    out_dir=os.path.join(outdir, \"kfold_tl_results\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU9oyviSAYbt",
        "outputId": "e9b4b4b1-2d1c-44e1-efc2-b6574d4c80e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold1.pt\n",
            "Fold 1: val R2=0.5317, mse=4.1938\n",
            "\n",
            "--- Fold 2/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold2.pt\n",
            "Fold 2: val R2=0.7274, mse=1.3607\n",
            "\n",
            "--- Fold 3/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold3.pt\n",
            "Fold 3: val R2=0.2757, mse=20.7231\n",
            "\n",
            "--- Fold 4/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold4.pt\n",
            "Fold 4: val R2=0.5194, mse=5.5238\n",
            "\n",
            "--- Fold 5/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold5.pt\n",
            "Fold 5: val R2=0.3752, mse=7.2138\n",
            "\n",
            "--- Fold 6/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold6.pt\n",
            "Fold 6: val R2=0.1504, mse=34.7134\n",
            "\n",
            "--- Fold 7/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold7.pt\n",
            "Fold 7: val R2=0.3006, mse=14.9029\n",
            "\n",
            "--- Fold 8/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold8.pt\n",
            "Fold 8: val R2=0.4579, mse=4.8830\n",
            "\n",
            "--- Fold 9/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold9.pt\n",
            "Fold 9: val R2=0.3370, mse=12.9468\n",
            "\n",
            "--- Fold 10/10 ---\n",
            "Saved fold model: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/kfold_fold10.pt\n",
            "Fold 10: val R2=0.3642, mse=5.0147\n",
            "Saved K-fold loss plot: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/transfer_learning_kfold_loss.png\n",
            "\n",
            "FINAL ENSEMBLE PERFORMANCE (on held-out Test set): R2=0.3023, MSE=9.6254\n",
            "Saved line-plot: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/transfer_learning_kfold_line_plot.png\n",
            "Saved time-series visualization: /content/drive/MyDrive/DNN_Wildfire/11_29_transfer_learning/kfold_tl_results/transfer_learning_kfold_time-series_prediction_visualization.png\n"
          ]
        }
      ]
    }
  ]
}