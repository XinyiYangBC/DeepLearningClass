{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQmN5MdRj-vJ",
        "outputId": "a977c61c-6240-4115-8f4a-6efc4d69ab69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.12.0)\n",
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.12/dist-packages (from xarray) (2.2.2)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.5 netCDF4-1.7.3\n",
            "using device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install xarray netCDF4 scipy torch numpy matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "import xarray as xr\n",
        "import joblib\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAlh9sjeeceM",
        "outputId": "0d1a315f-8f5c-42d1-e80e-b83f97d1f1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files in folder:\n",
            "  MC2_burnedFrac.nc4\n",
            "  JSBACH-SPITFIRE_burnedFrac.nc4\n",
            "  wildfire_surrogate4.mat\n",
            "  wildfire_surrogate6.mat\n",
            "  wildfire_surrogate9.mat\n",
            "  LPJ-GUESS-GlobFIRM_burnedFrac.nc4\n",
            "  JULES-INFERNO_burnedFrac.nc4\n",
            "  wildfire_surrogate7.mat\n",
            "  wildfire_surrogate8.mat\n",
            "  wildfire_surrogate2.mat\n",
            "  wildfire_surrogate5.mat\n",
            "  wildfire_surrogate14.mat\n",
            "  wildfire_surrogate11.mat\n",
            "  wildfire_surrogate1.mat\n",
            "  wildfire_surrogate10.mat\n",
            "  CTEM_burnedFrac.nc4\n",
            "  CLM_burnedFrac.nc4\n",
            "  LPJ-GUESS-SPITFIRE_burnedFrac.nc4\n",
            "  ORCHIDEE-SPITFIRE_burnedFrac.nc4\n",
            "  wildfire_surrogate3.mat\n",
            "  wildfire_surrogate13.mat\n",
            "  wildfire_surrogate12.mat\n",
            "  LPJ-GUESS-SIMFIRE-BLAZE_burnedFrac.nc4\n",
            "  11_17_own_data\n",
            "  11_22_transfer_learning\n",
            "  11_14_results\n",
            "  wildfire_surrogate1_DNN_softplus.pt\n",
            "  11_29_transfer_learning\n",
            "  global_wildfire_base_model.pt\n",
            "Loaded: CLM_burnedFrac.nc4\n",
            "Loaded: CTEM_burnedFrac.nc4\n",
            "Loaded: JSBACH-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: JULES-INFERNO_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-GlobFIRM_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-SIMFIRE-BLAZE_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: MC2_burnedFrac.nc4\n",
            "Loaded: ORCHIDEE-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: wildfire_surrogate1.mat\n",
            "Loaded: wildfire_surrogate10.mat\n",
            "Loaded: wildfire_surrogate11.mat\n",
            "Loaded: wildfire_surrogate12.mat\n",
            "Loaded: wildfire_surrogate13.mat\n",
            "Loaded: wildfire_surrogate14.mat\n",
            "Loaded: wildfire_surrogate2.mat\n",
            "Loaded: wildfire_surrogate3.mat\n",
            "Loaded: wildfire_surrogate4.mat\n",
            "Loaded: wildfire_surrogate5.mat\n",
            "Loaded: wildfire_surrogate6.mat\n",
            "Loaded: wildfire_surrogate7.mat\n",
            "Loaded: wildfire_surrogate8.mat\n",
            "Loaded: wildfire_surrogate9.mat\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/DNN_Wildfire/'\n",
        "mydir = data_folder\n",
        "print(\"Files in folder:\")\n",
        "for i in os.listdir(data_folder):\n",
        "  print(\" \", i)\n",
        "\n",
        "def get_path(filename):\n",
        "  return os.path.join(data_folder, filename)\n",
        "\n",
        "nc4_files = sorted([i for i in os.listdir(data_folder) if i.endswith('.nc4')])\n",
        "mat_files = sorted([i for i in os.listdir(data_folder) if i.endswith('.mat')])\n",
        "\n",
        "nc4_dataset = {}\n",
        "for file in nc4_files:\n",
        "  path = get_path(file)\n",
        "  ds = xr.open_dataset(path)\n",
        "  nc4_dataset[file] = ds\n",
        "  print(f\"Loaded: {file}\")\n",
        "\n",
        "mat_dataset = {}\n",
        "for file in mat_files:\n",
        "  path = get_path(file)\n",
        "  ds = loadmat(path)\n",
        "  mat_dataset[file] = ds\n",
        "  print(f\"Loaded: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRZ9FoxSncEo"
      },
      "outputs": [],
      "source": [
        "class DNNWildfire(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, 5)\n",
        "    self.fc2 = nn.Linear(5, 5)\n",
        "    self.fc3 = nn.Linear(5, 5)\n",
        "    self.fc4 = nn.Linear(5, 5)\n",
        "    self.fc5 = nn.Linear(5, 5)\n",
        "    self.fc6 = nn.Linear(5, 1)\n",
        "    self.softplus = nn.Softplus()\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, -0.05, 0.05)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    pred = self.softplus(self.fc1(x))\n",
        "    pred = self.softplus(self.fc2(pred))\n",
        "    pred = self.softplus(self.fc3(pred))\n",
        "    pred = self.softplus(self.fc4(pred))\n",
        "    pred = self.softplus(self.fc5(pred))\n",
        "    pred = self.fc6(pred)\n",
        "\n",
        "    if y is not None:\n",
        "      loss = nn.functional.mse_loss(pred, y)\n",
        "      return loss, pred\n",
        "    return pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_epoch(net, val_iter, device):\n",
        "  net.eval()\n",
        "  total_loss = 0.0\n",
        "  count = 0\n",
        "  with torch.no_grad():\n",
        "    for val_data in val_iter:\n",
        "      val_data = [ds.to(device) for ds in val_data]\n",
        "      loss, pred = net(*val_data)\n",
        "      total_loss += loss.mean().item()\n",
        "      count += 1\n",
        "  net.train()\n",
        "  return total_loss / count"
      ],
      "metadata": {
        "id": "dckswdURiZyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz_1GGhdDZvM"
      },
      "outputs": [],
      "source": [
        "def train_dnn(net, train_iter, val_iter, lr, epochs, device, decay_steps=1000, decay_rate=0.99):\n",
        "  net = net.to(device)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  train_loss_per_epoch = []\n",
        "  val_loss_per_epoch = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    total_train_loss_sum = 0.0\n",
        "    batch_count = 0\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      total_train_loss_sum += loss.mean().item()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      new_lr = lr * (decay_rate ** (step/decay_steps))\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[1][0].cpu()))\n",
        "\n",
        "    if batch_count > 0:\n",
        "        avg_train_loss = total_train_loss_sum / batch_count\n",
        "    else:\n",
        "        avg_train_loss = 0.0\n",
        "        print(f\"WARNING: Fold {e+1} had an empty training batch (batch_count=0).\")\n",
        "\n",
        "    train_loss_per_epoch.append(avg_train_loss)\n",
        "\n",
        "    val_loss = validate_epoch(net, val_iter, device)\n",
        "    val_loss_per_epoch.append(val_loss)\n",
        "  return train_loss_per_epoch, val_loss_per_epoch\n",
        "\n",
        "def predict_model(net, X, device):\n",
        "  net = net.to(device)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    X = X.to(device)\n",
        "    prediction = net(X).cpu().numpy()\n",
        "  return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDQYt-krC7B0"
      },
      "outputs": [],
      "source": [
        "def train_region(region_idx, mydir, device, force_retrain=False):\n",
        "  matfiles = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}.mat')\n",
        "  print(f\"\\n{'='*60}\")\n",
        "  print(f\"Processing region {region_idx+1}\")\n",
        "  print('='*60)\n",
        "\n",
        "  tmp = loadmat(matfiles)\n",
        "  ELMX = tmp.get('ELMX')\n",
        "  ELMy = tmp.get('ELMy')\n",
        "\n",
        "  sc_X = MinMaxScaler(feature_range=(0, 1))\n",
        "  sc_y = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "  X = sc_X.fit_transform(ELMX)\n",
        "  y = sc_y.fit_transform(ELMy.reshape(-1, 1))\n",
        "\n",
        "  scaler_filename_X = os.path.join(mydir, f\"scaler_X{region_idx+1}.mat\")\n",
        "  scaler_filename_y = os.path.join(mydir, f\"scaler_y{region_idx+1}.mat\")\n",
        "  joblib.dump(sc_X, scaler_filename_X)\n",
        "  joblib.dump(sc_y, scaler_filename_y)\n",
        "\n",
        "  y_1 = np.percentile(y, 33)\n",
        "  y_2 = np.percentile(y, 66)\n",
        "  y_3 = np.max(y)\n",
        "  strata_y = np.full([len(y), 1], 0)\n",
        "  for j in range(len(y)):\n",
        "    if y[j] <= y_1:\n",
        "      strata_y[j] = 1\n",
        "    elif y[j] <= y_2:\n",
        "      strata_y[j] = 2\n",
        "    elif y[j] <= y_3:\n",
        "      strata_y[j] = 3\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=strata_y, random_state=0)\n",
        "\n",
        "  train_dataset = TensorDataset(\n",
        "      torch.tensor(X_train, dtype=torch.float32),\n",
        "      torch.tensor(y_train, dtype=torch.float32)\n",
        "  )\n",
        "  train_iter = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "  model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus.pt')\n",
        "\n",
        "  net = DNNWildfire(input_dim=X_train.shape[1])\n",
        "\n",
        "  if os.path.exists(model_path) and not force_retrain:\n",
        "    print(f\"Loading model\")\n",
        "    net.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  else:\n",
        "    print(f\"Training model\")\n",
        "    loss_list = train_dnn(net, train_iter, lr=0.01, epochs=30, device=device, decay_steps=1000, decay_rate=0.99)\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "  X_all = sc_X.transform(ELMX)\n",
        "  X_all_tensor = torch.tensor(X_all, dtype=torch.float32)\n",
        "  y_pred = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  dnn_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 360)\n",
        "  data_y = sc_y.inverse_transform(y.reshape(-1, 1)).reshape(-1, 360)\n",
        "\n",
        "  return dnn_y, data_y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziBVqjZpI5U7"
      },
      "outputs": [],
      "source": [
        "def tune_region(region_idx, mydir, device, obs_name='ensemble'):\n",
        "  print(f\"\\n{'='*60}\")\n",
        "  print(f\"TUNING region {region_idx+1}\")\n",
        "  print(f\"{'='*60}\")\n",
        "\n",
        "  matfiles = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}.mat')\n",
        "  tmp = loadmat(matfiles)\n",
        "  OBSX = tmp.get('OBSX')\n",
        "  OBSy_gfed = tmp.get('OBSy')\n",
        "  OBSy_cci51 = tmp.get('OBSy_cci51')\n",
        "  OBSy_ccilt11 = tmp.get('OBSy_ccilt11')\n",
        "  OBSy_mcd64 = tmp.get('OBSy_mcd64')\n",
        "  OBSy_atlas = tmp.get('OBSy_atlas')\n",
        "\n",
        "  OBSy = np.mean(np.hstack((OBSy_gfed, OBSy_cci51, OBSy_ccilt11, OBSy_mcd64, OBSy_atlas)), axis=1).reshape(-1, 1)\n",
        "\n",
        "  scaler_filename_X = os.path.join(mydir, f\"scaler_X{region_idx+1}.mat\")\n",
        "  scaler_filename_y = os.path.join(mydir, f\"scaler_y{region_idx+1}.mat\")\n",
        "  sc_X = joblib.load(scaler_filename_X)\n",
        "  sc_y = joblib.load(scaler_filename_y)\n",
        "\n",
        "  X = sc_X.transform(OBSX)\n",
        "  y = sc_y.transform(OBSy.reshape(-1, 1))\n",
        "\n",
        "  y_1 = np.percentile(y, 33)\n",
        "  y_2 = np.percentile(y, 66)\n",
        "  y_3 = np.max(y)\n",
        "  strata_y = np.full([len(y), 1], 0)\n",
        "  for j in range(len(y)):\n",
        "    if y[j] <= y_1:\n",
        "      strata_y[j] = 1\n",
        "    elif y[j] <= y_2:\n",
        "      strata_y[j] = 2\n",
        "    elif y[j] <= y_3:\n",
        "      strata_y[j] = 3\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=strata_y, random_state=0)\n",
        "  train_dataset = TensorDataset(\n",
        "      torch.tensor(X_train, dtype=torch.float32),\n",
        "      torch.tensor(y_train, dtype=torch.float32)\n",
        "  )\n",
        "  train_iter = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "  base_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus.pt')\n",
        "  net = DNNWildfire(input_dim=X_train.shape[1])\n",
        "  print(f\"Loading base model: {base_model_path}\")\n",
        "  net.load_state_dict(torch.load(base_model_path, map_location=device))\n",
        "\n",
        "  tuned_model_path = ''\n",
        "  lr = 0.001\n",
        "  decay_steps = 1000\n",
        "  decay_rate = 0.9\n",
        "\n",
        "  if region_idx == 4 or region_idx == 8:\n",
        "    lr = 0.005\n",
        "    decay_steps = 3000\n",
        "    decay_rate = 0.99\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus_{obs_name}_tuned4.pt')\n",
        "  elif region_idx == 7:\n",
        "    lr = 0.005\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.99\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus_{obs_name}_tuned4.pt')\n",
        "  else:\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus_{obs_name}_tuned2.pt')\n",
        "\n",
        "  print(f\"Tuning model with learning rate = {lr}. Saving to: {tuned_model_path}\")\n",
        "  loss_list = train_dnn(net, train_iter, lr=lr, epochs=100, device=device, decay_steps=decay_steps, decay_rate=decay_rate)\n",
        "  torch.save(net.state_dict(), tuned_model_path)\n",
        "\n",
        "  #create tune_ensemble.csv files\n",
        "  X_all_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "  y_pred = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  dnn_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 120)\n",
        "  data_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 120)\n",
        "\n",
        "  return np.sum(dnn_y, 0), np.sum(data_y, 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_from_own_dataset(csv_path, mydir, device, lr=0.005, epochs=200, batch_size=8, test_size=0.20, random_state=0, use_log_transform=True, print_every=10):\n",
        "    print(f\"\\nLoad data from: {csv_path}\")\n",
        "    tmp = pd.read_csv(csv_path)\n",
        "\n",
        "    target_name = 'Burned_Area'\n",
        "    non_feature_cols = ['Date', target_name]\n",
        "    feature_names = [c for c in tmp.columns if c not in non_feature_cols]\n",
        "\n",
        "    print(f\"Found {len(feature_names)} features: \")\n",
        "    print(feature_names)\n",
        "\n",
        "    tmp = tmp.dropna(subset=feature_names + [target_name]).reset_index(drop=True)\n",
        "\n",
        "    all_dates = pd.to_datetime(tmp['Date'].values)\n",
        "\n",
        "    X = tmp[feature_names].values.astype(np.float32)\n",
        "    y = tmp[target_name].values.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "    if use_log_transform:\n",
        "        print(\"\\nApply log1p transform to target\")\n",
        "        y_trans = np.log1p(y)  # log(1 + y)\n",
        "        transform_method = 'log1p'\n",
        "    else:\n",
        "        print(\"\\nNo transform applied to target\")\n",
        "        y_trans = y.copy()\n",
        "        transform_method = 'none'\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X_scaled = sc_X.fit_transform(X)\n",
        "    y_scaled = sc_y.fit_transform(y_trans)\n",
        "\n",
        "    joblib.dump(sc_X, os.path.join(mydir, \"scaler_X_own_dataset.pkl\"))\n",
        "    joblib.dump(sc_y, os.path.join(mydir, \"scaler_y_own_dataset.pkl\"))\n",
        "    joblib.dump({'transform_method': transform_method}, os.path.join(mydir, \"preprocessing.pkl\"))\n",
        "\n",
        "    y_1 = np.percentile(y, 33)\n",
        "    y_2 = np.percentile(y, 66)\n",
        "    y_3 = np.max(y_scaled)\n",
        "    strata_y = np.full([len(y_scaled), 1], 0)\n",
        "    for j in range(len(y_scaled)):\n",
        "      if y_scaled[j] <= y_1:\n",
        "        strata_y[j] = 1\n",
        "      elif y_scaled[j] <= y_2:\n",
        "        strata_y[j] = 2\n",
        "      elif y_scaled[j] <= y_3:\n",
        "        strata_y[j] = 3\n",
        "\n",
        "    X_train, X_test, y_train, y_test, dates_train, dates_test = train_test_split(\n",
        "        X_scaled, y_scaled, all_dates, test_size=test_size, stratify=strata_y, random_state=random_state\n",
        "    )\n",
        "    print(f\"Train/Test sizes: {len(X_train)} / {len(X_test)}\")\n",
        "\n",
        "    batch_size_train = min(batch_size, max(1, len(X_train)))\n",
        "    batch_size_val = min(batch_size, len(X_test))\n",
        "\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.float32)\n",
        "    )\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.tensor(X_test, dtype=torch.float32),\n",
        "        torch.tensor(y_test, dtype=torch.float32)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "    net = DNNWildfire(input_dim).to(device)\n",
        "    print(f\"DEBUG: Calculated input_dim: {input_dim}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    # training loop\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = np.inf\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        batch_count = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, pred = net(xb, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        epoch_train_loss = running_loss / batch_count if batch_count > 0 else np.nan\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # validation\n",
        "        net.eval()\n",
        "        val_running = 0.0\n",
        "        val_count = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                loss_val, pred_val = net(xb, yb)\n",
        "                val_running += loss_val.item()\n",
        "                val_count += 1\n",
        "        epoch_val_loss = val_running / val_count if val_count > 0 else np.nan\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_state = net.state_dict()\n",
        "\n",
        "        if epoch % print_every == 0 or epoch == 1 or epoch == epochs:\n",
        "            print(f\"Epoch {epoch}/{epochs}  TrainLoss={epoch_train_loss:.6f}  ValLoss={epoch_val_loss:.6f}\")\n",
        "\n",
        "    # restore best model\n",
        "    if best_state is not None:\n",
        "        net.load_state_dict(best_state)\n",
        "\n",
        "    # save final model\n",
        "    model_path = os.path.join(mydir, 'own_dataset_wildfire_model_baseline.pt')\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "    print(\"Saved model to: \", model_path)\n",
        "\n",
        "    net.eval()\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        preds_scaled = net(X_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "    y_pred_transformed = sc_y.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
        "    y_test_transformed = sc_y.inverse_transform(y_test).flatten()\n",
        "\n",
        "    if use_log_transform:\n",
        "        y_pred_actual = np.expm1(y_pred_transformed)\n",
        "        y_test_actual = np.expm1(y_test_transformed)\n",
        "    else:\n",
        "        y_pred_actual = y_pred_transformed\n",
        "        y_test_actual = y_test_transformed\n",
        "\n",
        "    r2 = r2_score(y_test_actual, y_pred_actual)\n",
        "    mse = mean_squared_error(y_test_actual, y_pred_actual)\n",
        "\n",
        "    mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
        "    print(f\"\\nTest metrics — R2: {r2:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "    test_loss_transformed = mean_squared_error(y_test_transformed, y_pred_transformed)\n",
        "\n",
        "    # train/val loss plot\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(np.arange(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(1, len(val_losses)+1), val_losses, label='Val Loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title(f\"Train/Val Loss (R²={r2:.3f})\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(mydir, 'train_val_loss_baseline.png')\n",
        "    plt.savefig(plot_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Train/Val loss plot saved to: \", plot_path)\n",
        "\n",
        "    # scatter plot actual vs pred\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(y_test_actual, y_pred_actual, alpha=0.6, s=40, edgecolors='k', linewidths=0.4)\n",
        "    mn = min(y_test_actual.min(), y_pred_actual.min())\n",
        "    mx = max(y_test_actual.max(), y_pred_actual.max())\n",
        "    plt.plot([mn,mx],[mn,mx], 'r--', linewidth=1.5)\n",
        "    plt.xlabel('Actual Burned Area')\n",
        "    plt.ylabel('Predicted Burned Area')\n",
        "    plt.title(f\"Test — R²={r2:.3f}\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    scatter_path = os.path.join(mydir, 'test_scatter_plot_baseline.png')\n",
        "    plt.savefig(scatter_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Scatter plot saved to: \", scatter_path)\n",
        "\n",
        "    # line plot actual vs pred\n",
        "    plot_tmp = pd.DataFrame({\n",
        "        'Date': dates_test,\n",
        "        'Actual': y_test_actual,\n",
        "        'Predicted': y_pred_actual\n",
        "    })\n",
        "    plot_tmp = plot_tmp.sort_values(by='Date')\n",
        "    r2 = r2_score(plot_tmp['Actual'], plot_tmp['Predicted'])\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(plot_tmp['Date'], plot_tmp['Actual'], label=\"Actual Burned Area\", color='red', linewidth=2)\n",
        "    plt.plot(plot_tmp['Date'], plot_tmp['Predicted'], label=\"Predicted Burned Area\", color='blue', linestyle='--', linewidth=1.5)\n",
        "    plt.xlabel(\"Date\", fontsize=14, fontweight='bold')\n",
        "    plt.ylabel(f\"Burned Area (Mha)\", fontsize=14, fontweight='bold')\n",
        "    plt.title(f\"Model Time-Series Comparison on Full Dataset (R²: {r2:.3f})\", fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    line_path = os.path.join(mydir, 'time_series_performance_baseline.png')\n",
        "    plt.savefig(line_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Line plot saved to: \", line_path)\n",
        "\n",
        "    # time-series prediction visualization plot\n",
        "    N = len(y_test_actual)\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(range(N), y_test_actual, label='Actual Burned Area', color='blue', linewidth=2)\n",
        "    plt.plot(range(N), y_pred_actual, label='Predicted Burned Area', color='yellow', linewidth=2)\n",
        "    plt.title(f\"Time-Series Prediction Visualization (R² = {r2:.3f})\")\n",
        "    plt.legend()\n",
        "    visualization_path = os.path.join(mydir, 'time-series_prediction_visualization_baseline.png')\n",
        "    plt.savefig(visualization_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Time-series prediction visualization saved to: \", visualization_path)\n",
        "\n",
        "    history = {'train_losses': train_losses, 'val_losses': val_losses}\n",
        "\n",
        "    return net, sc_X, sc_y, history, y_test_actual, y_pred_actual"
      ],
      "metadata": {
        "id": "bbKkN72O0I-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "own_data_folder = '/content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning'\n",
        "mydir_own_data = own_data_folder\n",
        "\n",
        "csv_files_list = sorted([i for i in os.listdir(mydir_own_data) if i.endswith('dataset.csv')])\n",
        "\n",
        "if len(csv_files_list) > 0:\n",
        "    own_dataset_csv_filename = csv_files_list[0]\n",
        "    own_data_csv_path = os.path.join(mydir_own_data, own_dataset_csv_filename)\n",
        "\n",
        "    print(f\"Found dataset: {own_data_csv_path}\")\n",
        "\n",
        "    trained_net, sc_X, sc_y, history, y_test_actual, y_pred_actual = train_model_from_own_dataset(own_data_csv_path, mydir_own_data, device)\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: No file ending in 'dataset.csv' was found in {mydir_own_data}\")"
      ],
      "metadata": {
        "id": "QBACamCbyN2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "57a7fb98-f18d-4d45-a420-64844bebb048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found dataset: /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/DeepLearning_Climate_Biomass_Human_Fire_dataset_2001_2024_montly_dataset.csv\n",
            "\n",
            "Load data from: /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/DeepLearning_Climate_Biomass_Human_Fire_dataset_2001_2024_montly_dataset.csv\n",
            "Found 16 features: \n",
            "['Precipitation', 'Surface_Temperature', 'Wind_Speed', 'VPD', 'Specific_Humidity', 'Soil_Moisture', 'EVI', 'NDVI', 'LAI', 'Cropland_Area', 'Pasture_Area', 'Rangeland_Area', 'Urban_Area', 'Population_Total', 'Population_Rural', 'Population_Urban']\n",
            "\n",
            "Apply log1p transform to target\n",
            "Train/Test sizes: 230 / 58\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Epoch 1/200  TrainLoss=1.028102  ValLoss=0.933620\n",
            "Epoch 10/200  TrainLoss=0.414194  ValLoss=0.360134\n",
            "Epoch 20/200  TrainLoss=0.259237  ValLoss=0.307446\n",
            "Epoch 30/200  TrainLoss=0.230277  ValLoss=0.313082\n",
            "Epoch 40/200  TrainLoss=0.231079  ValLoss=0.287010\n",
            "Epoch 50/200  TrainLoss=0.225837  ValLoss=0.286785\n",
            "Epoch 60/200  TrainLoss=0.204385  ValLoss=0.243554\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-991973928.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found dataset: {own_data_csv_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrained_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_from_own_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mown_data_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydir_own_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-276187219.py\u001b[0m in \u001b[0;36mtrain_model_from_own_dataset\u001b[0;34m(csv_path, mydir, device, lr, epochs, batch_size, test_size, random_state, use_log_transform, print_every)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for own dataset\n",
        "csv_files = sorted([i for i in os.listdir(mydir_own_data) if i.endswith('dataset.csv')])\n",
        "if not csv_files:\n",
        "  print(\"ERROR: No CSV file found to plot.\")\n",
        "else:\n",
        "  csv_path = os.path.join(mydir_own_data, csv_files[0])\n",
        "  tmp = pd.read_csv(csv_path).dropna()\n",
        "\n",
        "  target_name = 'Burned_Area'\n",
        "  non_feature_columns = ['Date', target_name]\n",
        "\n",
        "  all_columns = tmp.columns.tolist()\n",
        "\n",
        "  feature_names = [col for col in all_columns if col not in non_feature_columns]\n",
        "\n",
        "  sc_X = joblib.load(os.path.join(mydir_own_data, \"scaler_X_own_dataset.pkl\"))\n",
        "  sc_y = joblib.load(os.path.join(mydir_own_data, \"scaler_y_own_dataset.pkl\"))\n",
        "\n",
        "  net = DNNWildfire(input_dim=len(feature_names))\n",
        "  net.load_state_dict(torch.load(os.path.join(mydir_own_data, \"own_dataset_wildfire_model_baseline.pt\"), map_location=device))\n",
        "\n",
        "  X_all = tmp[feature_names].values\n",
        "  X_all_scaled = sc_X.transform(X_all)\n",
        "  X_all_tensor = torch.tensor(X_all_scaled, dtype=torch.float32)\n",
        "  y_pred_scaled = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  y_pred_actual = sc_y.inverse_transform(y_pred_scaled)\n",
        "  y_actual = tmp[target_name].values\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.scatter(y_actual, y_pred_actual, alpha=0.3, label=\"Data Points\")\n",
        "  plt.plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'r--', label=\"Perfect Match (1:1 Line)\")\n",
        "  plt.xlabel(f\"Actual {target_name}\")\n",
        "  plt.ylabel(f\"Predicted {target_name}\")\n",
        "  plt.title(\"Model Performance\")\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(mydir_own_data, \"own_data_model_performance_baseline.png\"))\n",
        "  plt.clf()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "RMPCFwpf0bI-",
        "outputId": "579b8c39-c7f6-4881-f949-d27f969c1dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}