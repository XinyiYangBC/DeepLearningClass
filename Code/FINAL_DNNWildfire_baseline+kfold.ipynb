{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQmN5MdRj-vJ",
        "outputId": "5f1eb59e-aed4-42e3-a5d7-67179f815a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.12.0)\n",
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.12/dist-packages (from xarray) (2.2.2)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.5 netCDF4-1.7.3\n",
            "using device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install xarray netCDF4 scipy torch numpy matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "import xarray as xr\n",
        "import joblib\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAlh9sjeeceM",
        "outputId": "7f4e2c20-4223-49db-c914-a6838429a8d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files in folder:\n",
            "  MC2_burnedFrac.nc4\n",
            "  JSBACH-SPITFIRE_burnedFrac.nc4\n",
            "  wildfire_surrogate4.mat\n",
            "  wildfire_surrogate6.mat\n",
            "  wildfire_surrogate9.mat\n",
            "  LPJ-GUESS-GlobFIRM_burnedFrac.nc4\n",
            "  JULES-INFERNO_burnedFrac.nc4\n",
            "  wildfire_surrogate7.mat\n",
            "  wildfire_surrogate8.mat\n",
            "  wildfire_surrogate2.mat\n",
            "  wildfire_surrogate5.mat\n",
            "  wildfire_surrogate14.mat\n",
            "  wildfire_surrogate11.mat\n",
            "  wildfire_surrogate1.mat\n",
            "  wildfire_surrogate10.mat\n",
            "  CTEM_burnedFrac.nc4\n",
            "  CLM_burnedFrac.nc4\n",
            "  LPJ-GUESS-SPITFIRE_burnedFrac.nc4\n",
            "  ORCHIDEE-SPITFIRE_burnedFrac.nc4\n",
            "  wildfire_surrogate3.mat\n",
            "  wildfire_surrogate13.mat\n",
            "  wildfire_surrogate12.mat\n",
            "  LPJ-GUESS-SIMFIRE-BLAZE_burnedFrac.nc4\n",
            "  11_17_own_data\n",
            "  11_22_transfer_learning\n",
            "  11_14_results\n",
            "  wildfire_surrogate1_DNN_softplus.pt\n",
            "  11_29_transfer_learning\n",
            "  global_wildfire_base_model.pt\n",
            "Loaded: CLM_burnedFrac.nc4\n",
            "Loaded: CTEM_burnedFrac.nc4\n",
            "Loaded: JSBACH-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: JULES-INFERNO_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-GlobFIRM_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-SIMFIRE-BLAZE_burnedFrac.nc4\n",
            "Loaded: LPJ-GUESS-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: MC2_burnedFrac.nc4\n",
            "Loaded: ORCHIDEE-SPITFIRE_burnedFrac.nc4\n",
            "Loaded: wildfire_surrogate1.mat\n",
            "Loaded: wildfire_surrogate10.mat\n",
            "Loaded: wildfire_surrogate11.mat\n",
            "Loaded: wildfire_surrogate12.mat\n",
            "Loaded: wildfire_surrogate13.mat\n",
            "Loaded: wildfire_surrogate14.mat\n",
            "Loaded: wildfire_surrogate2.mat\n",
            "Loaded: wildfire_surrogate3.mat\n",
            "Loaded: wildfire_surrogate4.mat\n",
            "Loaded: wildfire_surrogate5.mat\n",
            "Loaded: wildfire_surrogate6.mat\n",
            "Loaded: wildfire_surrogate7.mat\n",
            "Loaded: wildfire_surrogate8.mat\n",
            "Loaded: wildfire_surrogate9.mat\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/DNN_Wildfire/'\n",
        "mydir = data_folder\n",
        "print(\"Files in folder:\")\n",
        "for i in os.listdir(data_folder):\n",
        "  print(\" \", i)\n",
        "\n",
        "def get_path(filename):\n",
        "  return os.path.join(data_folder, filename)\n",
        "\n",
        "nc4_files = sorted([i for i in os.listdir(data_folder) if i.endswith('.nc4')])\n",
        "mat_files = sorted([i for i in os.listdir(data_folder) if i.endswith('.mat')])\n",
        "\n",
        "nc4_dataset = {}\n",
        "for file in nc4_files:\n",
        "  path = get_path(file)\n",
        "  ds = xr.open_dataset(path)\n",
        "  nc4_dataset[file] = ds\n",
        "  print(f\"Loaded: {file}\")\n",
        "\n",
        "mat_dataset = {}\n",
        "for file in mat_files:\n",
        "  path = get_path(file)\n",
        "  ds = loadmat(path)\n",
        "  mat_dataset[file] = ds\n",
        "  print(f\"Loaded: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRZ9FoxSncEo"
      },
      "outputs": [],
      "source": [
        "class DNNWildfire(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, 5)\n",
        "    self.fc2 = nn.Linear(5, 5)\n",
        "    self.fc3 = nn.Linear(5, 5)\n",
        "    self.fc4 = nn.Linear(5, 5)\n",
        "    self.fc5 = nn.Linear(5, 5)\n",
        "    self.fc6 = nn.Linear(5, 1)\n",
        "    self.softplus = nn.Softplus()\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, -0.05, 0.05)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    pred = self.softplus(self.fc1(x))\n",
        "    pred = self.softplus(self.fc2(pred))\n",
        "    pred = self.softplus(self.fc3(pred))\n",
        "    pred = self.softplus(self.fc4(pred))\n",
        "    pred = self.softplus(self.fc5(pred))\n",
        "    pred = self.fc6(pred)\n",
        "\n",
        "    if y is not None:\n",
        "      loss = nn.functional.mse_loss(pred, y)\n",
        "      return loss, pred\n",
        "    return pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_epoch(net, val_iter, device):\n",
        "  net.eval()\n",
        "  total_loss = 0.0\n",
        "  count = 0\n",
        "  with torch.no_grad():\n",
        "    for val_data in val_iter:\n",
        "      val_data = [ds.to(device) for ds in val_data]\n",
        "      loss, pred = net(*val_data)\n",
        "      total_loss += loss.mean().item()\n",
        "      count += 1\n",
        "  net.train()\n",
        "  return total_loss / count"
      ],
      "metadata": {
        "id": "dckswdURiZyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz_1GGhdDZvM"
      },
      "outputs": [],
      "source": [
        "def train_dnn(net, train_iter, val_iter, lr, epochs, device, decay_steps=1000, decay_rate=0.99):\n",
        "  net = net.to(device)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  train_loss_per_epoch = []\n",
        "  val_loss_per_epoch = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    total_train_loss_sum = 0.0\n",
        "    batch_count = 0\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      total_train_loss_sum += loss.mean().item()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      new_lr = lr * (decay_rate ** (step/decay_steps))\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[1][0].cpu()))\n",
        "\n",
        "    if batch_count > 0:\n",
        "        avg_train_loss = total_train_loss_sum / batch_count\n",
        "    else:\n",
        "        avg_train_loss = 0.0\n",
        "        print(f\"WARNING: Fold {e+1} had an empty training batch (batch_count=0).\")\n",
        "\n",
        "    train_loss_per_epoch.append(avg_train_loss)\n",
        "\n",
        "    val_loss = validate_epoch(net, val_iter, device)\n",
        "    val_loss_per_epoch.append(val_loss)\n",
        "  return train_loss_per_epoch, val_loss_per_epoch\n",
        "\n",
        "def predict_model(net, X, device):\n",
        "  net = net.to(device)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    X = X.to(device)\n",
        "    prediction = net(X).cpu().numpy()\n",
        "  return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDQYt-krC7B0"
      },
      "outputs": [],
      "source": [
        "def train_region(region_idx, mydir, device, force_retrain=False):\n",
        "  matfiles = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}.mat')\n",
        "  print(f\"\\n{'='*60}\")\n",
        "  print(f\"Processing region {region_idx+1}\")\n",
        "  print('='*60)\n",
        "\n",
        "  tmp = loadmat(matfiles)\n",
        "  ELMX = tmp.get('ELMX')\n",
        "  ELMy = tmp.get('ELMy')\n",
        "\n",
        "  sc_X = MinMaxScaler(feature_range=(0, 1))\n",
        "  sc_y = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "  X = sc_X.fit_transform(ELMX)\n",
        "  y = sc_y.fit_transform(ELMy.reshape(-1, 1))\n",
        "\n",
        "  scaler_filename_X = os.path.join(mydir, f\"scaler_X{region_idx+1}.mat\")\n",
        "  scaler_filename_y = os.path.join(mydir, f\"scaler_y{region_idx+1}.mat\")\n",
        "  joblib.dump(sc_X, scaler_filename_X)\n",
        "  joblib.dump(sc_y, scaler_filename_y)\n",
        "\n",
        "  y_1 = np.percentile(y, 33)\n",
        "  y_2 = np.percentile(y, 66)\n",
        "  y_3 = np.max(y)\n",
        "  strata_y = np.full([len(y), 1], 0)\n",
        "  for j in range(len(y)):\n",
        "    if y[j] <= y_1:\n",
        "      strata_y[j] = 1\n",
        "    elif y[j] <= y_2:\n",
        "      strata_y[j] = 2\n",
        "    elif y[j] <= y_3:\n",
        "      strata_y[j] = 3\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=strata_y, random_state=0)\n",
        "\n",
        "  train_dataset = TensorDataset(\n",
        "      torch.tensor(X_train, dtype=torch.float32),\n",
        "      torch.tensor(y_train, dtype=torch.float32)\n",
        "  )\n",
        "  train_iter = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "  model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus.pt')\n",
        "\n",
        "  net = DNNWildfire(input_dim=X_train.shape[1])\n",
        "\n",
        "  if os.path.exists(model_path) and not force_retrain:\n",
        "    print(f\"Loading model\")\n",
        "    net.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  else:\n",
        "    print(f\"Training model\")\n",
        "    loss_list = train_dnn(net, train_iter, lr=0.01, epochs=30, device=device, decay_steps=1000, decay_rate=0.99)\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "  X_all = sc_X.transform(ELMX)\n",
        "  X_all_tensor = torch.tensor(X_all, dtype=torch.float32)\n",
        "  y_pred = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  dnn_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 360)\n",
        "  data_y = sc_y.inverse_transform(y.reshape(-1, 1)).reshape(-1, 360)\n",
        "\n",
        "  return dnn_y, data_y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziBVqjZpI5U7"
      },
      "outputs": [],
      "source": [
        "def tune_region(region_idx, mydir, device, obs_name='ensemble'):\n",
        "  print(f\"\\n{'='*60}\")\n",
        "  print(f\"TUNING region {region_idx+1}\")\n",
        "  print(f\"{'='*60}\")\n",
        "\n",
        "  matfiles = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}.mat')\n",
        "  tmp = loadmat(matfiles)\n",
        "  OBSX = tmp.get('OBSX')\n",
        "  OBSy_gfed = tmp.get('OBSy')\n",
        "  OBSy_cci51 = tmp.get('OBSy_cci51')\n",
        "  OBSy_ccilt11 = tmp.get('OBSy_ccilt11')\n",
        "  OBSy_mcd64 = tmp.get('OBSy_mcd64')\n",
        "  OBSy_atlas = tmp.get('OBSy_atlas')\n",
        "\n",
        "  OBSy = np.mean(np.hstack((OBSy_gfed, OBSy_cci51, OBSy_ccilt11, OBSy_mcd64, OBSy_atlas)), axis=1).reshape(-1, 1)\n",
        "\n",
        "  scaler_filename_X = os.path.join(mydir, f\"scaler_X{region_idx+1}.mat\")\n",
        "  scaler_filename_y = os.path.join(mydir, f\"scaler_y{region_idx+1}.mat\")\n",
        "  sc_X = joblib.load(scaler_filename_X)\n",
        "  sc_y = joblib.load(scaler_filename_y)\n",
        "\n",
        "  X = sc_X.transform(OBSX)\n",
        "  y = sc_y.transform(OBSy.reshape(-1, 1))\n",
        "\n",
        "  y_1 = np.percentile(y, 33)\n",
        "  y_2 = np.percentile(y, 66)\n",
        "  y_3 = np.max(y)\n",
        "  strata_y = np.full([len(y), 1], 0)\n",
        "  for j in range(len(y)):\n",
        "    if y[j] <= y_1:\n",
        "      strata_y[j] = 1\n",
        "    elif y[j] <= y_2:\n",
        "      strata_y[j] = 2\n",
        "    elif y[j] <= y_3:\n",
        "      strata_y[j] = 3\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=strata_y, random_state=0)\n",
        "  train_dataset = TensorDataset(\n",
        "      torch.tensor(X_train, dtype=torch.float32),\n",
        "      torch.tensor(y_train, dtype=torch.float32)\n",
        "  )\n",
        "  train_iter = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "  base_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus.pt')\n",
        "  net = DNNWildfire(input_dim=X_train.shape[1])\n",
        "  print(f\"Loading base model: {base_model_path}\")\n",
        "  net.load_state_dict(torch.load(base_model_path, map_location=device))\n",
        "\n",
        "  tuned_model_path = ''\n",
        "  lr = 0.001\n",
        "  decay_steps = 1000\n",
        "  decay_rate = 0.9\n",
        "\n",
        "  if region_idx == 4 or region_idx == 8:\n",
        "    lr = 0.005\n",
        "    decay_steps = 3000\n",
        "    decay_rate = 0.99\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus_{obs_name}_tuned4.pt')\n",
        "  elif region_idx == 7:\n",
        "    lr = 0.005\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.99\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus_{obs_name}_tuned4.pt')\n",
        "  else:\n",
        "    tuned_model_path = os.path.join(mydir, f'wildfire_surrogate{region_idx+1}_DNN_softplus_{obs_name}_tuned2.pt')\n",
        "\n",
        "  print(f\"Tuning model with learning rate = {lr}. Saving to: {tuned_model_path}\")\n",
        "  loss_list = train_dnn(net, train_iter, lr=lr, epochs=100, device=device, decay_steps=decay_steps, decay_rate=decay_rate)\n",
        "  torch.save(net.state_dict(), tuned_model_path)\n",
        "\n",
        "  X_all_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "  y_pred = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  dnn_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 120)\n",
        "  data_y = sc_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1, 120)\n",
        "\n",
        "  return np.sum(dnn_y, 0), np.sum(data_y, 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_from_own_dataset(csv_path, mydir, device, n_splits=10, epochs=100, lr=0.005, batch_size=8, random_state=0, use_log_transform=True):\n",
        "    print(f\"\\nLoad data from: {csv_path}\")\n",
        "    tmp = pd.read_csv(csv_path)\n",
        "\n",
        "    target_name = 'Burned_Area'\n",
        "    non_feature_cols = ['Date', target_name]\n",
        "    feature_names = [c for c in tmp.columns if c not in non_feature_cols]\n",
        "\n",
        "    print(f\"Found {len(feature_names)} features: \")\n",
        "    print(feature_names)\n",
        "\n",
        "    tmp = tmp.dropna(subset=feature_names + [target_name]).reset_index(drop=True)\n",
        "\n",
        "    all_dates = pd.to_datetime(tmp['Date'].values)\n",
        "\n",
        "    X = tmp[feature_names].values.astype(float)\n",
        "    y = tmp[target_name].values.astype(float).reshape(-1, 1)\n",
        "\n",
        "    if use_log_transform:\n",
        "        print(\"\\nApply log1p transform to target\")\n",
        "        y_trans = np.log1p(y)  # log(1 + y)\n",
        "    else:\n",
        "        print(\"\\nNo transform applied to target\")\n",
        "        y_trans = y.copy()\n",
        "\n",
        "    sc_X = StandardScaler().fit(X)\n",
        "    sc_y = StandardScaler().fit(y_trans)\n",
        "\n",
        "    X_scaled = sc_X.fit_transform(X)\n",
        "    y_scaled = sc_y.fit_transform(y_trans)\n",
        "\n",
        "    y_1 = np.percentile(y, 33)\n",
        "    y_2 = np.percentile(y, 66)\n",
        "    y_3 = np.max(y_scaled)\n",
        "    strata_y = np.full([len(y_scaled), 1], 0)\n",
        "    for j in range(len(y_scaled)):\n",
        "      if y_scaled[j] <= y_1:\n",
        "        strata_y[j] = 1\n",
        "      elif y_scaled[j] <= y_2:\n",
        "        strata_y[j] = 2\n",
        "      elif y_scaled[j] <= y_3:\n",
        "        strata_y[j] = 3\n",
        "\n",
        "    X_kfold, X_test, y_kfold, y_test, dates_kfold, dates_test, strata_kfold, strata_test = train_test_split(\n",
        "        X_scaled, y_scaled, all_dates, strata_y, test_size=0.20, stratify=strata_y, random_state=random_state\n",
        "    )\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    fold_metrics = []\n",
        "    all_preds_val = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_kfold, strata_kfold), start=1):\n",
        "      print(f\"\\nFold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}\")\n",
        "      X_train, X_val = X_kfold[train_idx], X_kfold[val_idx]\n",
        "      y_train, y_val = y_kfold[train_idx], y_kfold[val_idx]\n",
        "      dates_train, dates_val = dates_kfold[train_idx], dates_kfold[val_idx]\n",
        "\n",
        "      if len(X_train) < 2:\n",
        "        print(f\"Skipping fold {fold} due to training set being too small\")\n",
        "        continue\n",
        "\n",
        "      batch_size_train = min(batch_size, max(1, len(X_train)))\n",
        "      batch_size_val = min(batch_size, len(X_val))\n",
        "      train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.float32)\n",
        "      )\n",
        "      val_dataset = TensorDataset(\n",
        "        torch.tensor(X_val, dtype=torch.float32),\n",
        "        torch.tensor(y_val, dtype=torch.float32)\n",
        "      )\n",
        "      train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "      val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
        "      input_dim = X.shape[1]\n",
        "      net = DNNWildfire(input_dim).to(device)\n",
        "      print(f\"DEBUG: Calculated input_dim: {input_dim}\")\n",
        "      optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "      train_losses = []\n",
        "      val_losses = []\n",
        "      best_val_loss = np.inf\n",
        "      best_state = None\n",
        "      for epoch in range(1, epochs+1):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        batch_count = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, pred = net(xb, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        epoch_train_loss = running_loss / batch_count if batch_count > 0 else np.nan\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # validation\n",
        "        net.eval()\n",
        "        val_running = 0.0\n",
        "        val_count = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                loss_val, pred_val = net(xb, yb)\n",
        "                val_running += loss_val.item()\n",
        "                val_count += 1\n",
        "        epoch_val_loss = val_running / val_count if val_count > 0 else np.nan\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_state = net.state_dict()\n",
        "\n",
        "      # restore best model\n",
        "      if best_state is not None:\n",
        "        net.load_state_dict(best_state)\n",
        "      # save fold model\n",
        "      fold_model_path = os.path.join(mydir, f'own_dataset_kfold_f{fold}_model.pt')\n",
        "      torch.save(net.state_dict(), fold_model_path)\n",
        "      print(\"Saved fold model to: \", fold_model_path)\n",
        "\n",
        "      net.eval()\n",
        "      X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "      with torch.no_grad():\n",
        "        preds_val = net(X_val_tensor).cpu().numpy().flatten()\n",
        "      all_preds_val.append(preds_val)\n",
        "\n",
        "      y_val_transformed = sc_y.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
        "      preds_val_transformed = sc_y.inverse_transform(preds_val.reshape(-1, 1)).flatten()\n",
        "      if use_log_transform:\n",
        "        y_val_actual = np.expm1(y_val_transformed)\n",
        "        preds_val_actual = np.expm1(preds_val_transformed)\n",
        "      else:\n",
        "        y_val_actual = y_val_transformed\n",
        "        preds_val_actual = preds_val_transformed\n",
        "\n",
        "      r2 = r2_score(y_val_actual, preds_val_actual)\n",
        "      mse = mean_squared_error(y_val_actual, preds_val_actual)\n",
        "      fold_metrics.append({'fold': fold, 'r2': r2, 'mse': mse, 'train_losses': train_losses, 'val_losses': val_losses})\n",
        "    print(\"\\nK-Fold cross validation completed\")\n",
        "\n",
        "    # k-fold loss plot\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for m in fold_metrics:\n",
        "      fold = m['fold']\n",
        "      r2 = m['r2']\n",
        "      trainloss = m['train_losses']\n",
        "      valloss = m['val_losses']\n",
        "      epochs = np.arange(1, len(trainloss)+1)\n",
        "      plt.plot(epochs, trainloss, label=f\"Fold {fold} - Train (R² = {r2:.3f})\", alpha=0.6)\n",
        "      plt.plot(epochs, valloss, label=f\"Fold {fold} - Validation (R² = {r2:.3f})\", alpha=0.6, linestyle='--')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"K-Fold Loss Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    kfold_path = os.path.join(mydir, 'k_fold_model_loss.png')\n",
        "    plt.savefig(kfold_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"K-Fold loss plot saved to: \", kfold_path)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    all_fold_predictions = []\n",
        "\n",
        "    print(f\"Running Ensemble Prediction on {len(X_test)} Test Samples\")\n",
        "    for fold in range(1, n_splits + 1):\n",
        "        net_fold = DNNWildfire(input_dim=X.shape[1]).to(device)\n",
        "        model_path = os.path.join(mydir, f'own_dataset_kfold_f{fold}_model.pt')\n",
        "        net_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "        net_fold.eval()\n",
        "        with torch.no_grad():\n",
        "          preds_fold = net_fold(X_test_tensor).cpu().numpy().flatten()\n",
        "          all_fold_predictions.append(preds_fold)\n",
        "\n",
        "    avg_preds_kfold = np.mean(all_fold_predictions, axis=0)\n",
        "    y_pred_transformed = sc_y.inverse_transform(avg_preds_kfold.reshape(-1, 1)).flatten()\n",
        "    y_test_transformed = sc_y.inverse_transform(y_test).flatten()\n",
        "\n",
        "    if use_log_transform:\n",
        "      y_pred_actual = np.expm1(y_pred_transformed)\n",
        "      y_test_actual = np.expm1(y_test_transformed)\n",
        "    else:\n",
        "      y_pred_actual = y_pred_transformed\n",
        "      y_test_actual = y_test_transformed\n",
        "\n",
        "    final_r2 = r2_score(y_test_actual, y_pred_actual)\n",
        "\n",
        "    print(f\"\\nFINAL ENSEMBLE PERFORMANCE (on held-out Test Set):\")\n",
        "    print(f\"R-squared (R²) Score: {final_r2:.3f}\")\n",
        "\n",
        "    # line plot actual vs pred\n",
        "    plot_tmp = pd.DataFrame({\n",
        "        'Date': dates_test,\n",
        "        'Actual': y_test_actual,\n",
        "        'Predicted': y_pred_actual\n",
        "    })\n",
        "    plot_tmp = plot_tmp.sort_values(by='Date')\n",
        "    r2 = r2_score(plot_tmp['Actual'], plot_tmp['Predicted'])\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(plot_tmp['Date'], plot_tmp['Actual'], label=\"Actual Burned Area\", color='red', linewidth=2)\n",
        "    plt.plot(plot_tmp['Date'], plot_tmp['Predicted'], label=\"Predicted Burned Area\", color='blue', linestyle='--', linewidth=1.5)\n",
        "    plt.xlabel(\"Date\", fontsize=14, fontweight='bold')\n",
        "    plt.ylabel(f\"Burned Area (Mha)\", fontsize=14, fontweight='bold')\n",
        "    plt.title(f\"Model Time-Series Comparison on Full Dataset (R²: {r2:.3f})\", fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    line_path = os.path.join(mydir, 'kfold_model_line_plot_actual_vs_pred.png')\n",
        "    plt.savefig(line_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Line plot saved to: \", line_path)\n",
        "\n",
        "    # time-series prediction visualization plot\n",
        "    N = len(y_test_actual)\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(range(N), y_test_actual, label='Actual Burned Area', color='blue', linewidth=2)\n",
        "    plt.plot(range(N), y_pred_actual, label='Predicted Burned Area', color='yellow', linewidth=2)\n",
        "    plt.title(f\"Time-Series Prediction Visualization (R² = {r2:.3f})\")\n",
        "    plt.legend()\n",
        "    visualization_path = os.path.join(mydir, 'kfold_model_time-series_prediction_visualization.png')\n",
        "    plt.savefig(visualization_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(\"Time-series prediction visualization saved to: \", visualization_path)\n",
        "\n",
        "\n",
        "    return net, {'folds': fold_metrics, 'scalers': (sc_X, sc_y)}\n"
      ],
      "metadata": {
        "id": "bbKkN72O0I-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "own_data_folder = '/content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning'\n",
        "mydir_own_data = own_data_folder\n",
        "\n",
        "csv_files_list = sorted([i for i in os.listdir(mydir_own_data) if i.endswith('dataset.csv')])\n",
        "\n",
        "if len(csv_files_list) > 0:\n",
        "    own_dataset_csv_filename = csv_files_list[0]\n",
        "    own_data_csv_path = os.path.join(mydir_own_data, own_dataset_csv_filename)\n",
        "\n",
        "    print(f\"Found dataset: {own_data_csv_path}\")\n",
        "\n",
        "    net, fold_metrics = train_model_from_own_dataset(own_data_csv_path, mydir_own_data, device)\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: No file ending in 'dataset.csv' was found in {mydir_own_data}\")"
      ],
      "metadata": {
        "id": "QBACamCbyN2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6359a39-6547-4ac4-f579-9c203be571ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found dataset: /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/DeepLearning_Climate_Biomass_Human_Fire_dataset_2001_2024_montly_dataset.csv\n",
            "\n",
            "Load data from: /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/DeepLearning_Climate_Biomass_Human_Fire_dataset_2001_2024_montly_dataset.csv\n",
            "Found 16 features: \n",
            "['Precipitation', 'Surface_Temperature', 'Wind_Speed', 'VPD', 'Specific_Humidity', 'Soil_Moisture', 'EVI', 'NDVI', 'LAI', 'Cropland_Area', 'Pasture_Area', 'Rangeland_Area', 'Urban_Area', 'Population_Total', 'Population_Rural', 'Population_Urban']\n",
            "\n",
            "Apply log1p transform to target\n",
            "\n",
            "Fold 1/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f1_model.pt\n",
            "\n",
            "Fold 2/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f2_model.pt\n",
            "\n",
            "Fold 3/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f3_model.pt\n",
            "\n",
            "Fold 4/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f4_model.pt\n",
            "\n",
            "Fold 5/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f5_model.pt\n",
            "\n",
            "Fold 6/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f6_model.pt\n",
            "\n",
            "Fold 7/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f7_model.pt\n",
            "\n",
            "Fold 8/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f8_model.pt\n",
            "\n",
            "Fold 9/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f9_model.pt\n",
            "\n",
            "Fold 10/10: train=207, val=23\n",
            "DEBUG: Calculated input_dim: 16\n",
            "Saved fold model to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/own_dataset_kfold_f10_model.pt\n",
            "\n",
            "K-Fold cross validation completed\n",
            "K-Fold loss plot saved to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/k_fold_model_loss.png\n",
            "Running Ensemble Prediction on 58 Test Samples\n",
            "\n",
            "FINAL ENSEMBLE PERFORMANCE (on held-out Test Set):\n",
            "R-squared (R²) Score: 0.737\n",
            "Line plot saved to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/kfold_model_line_plot_actual_vs_pred.png\n",
            "Time-series prediction visualization saved to:  /content/drive/MyDrive/DNN_Wildfire/11_22_transfer_learning/kfold_model_time-series_prediction_visualization.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for own dataset\n",
        "csv_files = sorted([i for i in os.listdir(mydir_own_data) if i.endswith('dataset.csv')])\n",
        "if not csv_files:\n",
        "  print(\"ERROR: No CSV file found to plot.\")\n",
        "else:\n",
        "  csv_path = os.path.join(mydir_own_data, csv_files[0])\n",
        "  tmp = pd.read_csv(csv_path).dropna()\n",
        "\n",
        "  target_name = 'Burned_Area'\n",
        "  non_feature_columns = ['Date', target_name]\n",
        "\n",
        "  all_columns = tmp.columns.tolist()\n",
        "\n",
        "  feature_names = [col for col in all_columns if col not in non_feature_columns]\n",
        "\n",
        "  sc_X = joblib.load(os.path.join(mydir_own_data, \"scaler_X_own_dataset.pkl\"))\n",
        "  sc_y = joblib.load(os.path.join(mydir_own_data, \"scaler_y_own_dataset.pkl\"))\n",
        "\n",
        "  net = DNNWildfire(input_dim=len(feature_names))\n",
        "  net.load_state_dict(torch.load(os.path.join(mydir_own_data, \"own_dataset_wildfire_model.pt\"), map_location=device))\n",
        "\n",
        "  X_all = tmp[feature_names].values\n",
        "  X_all_scaled = sc_X.transform(X_all)\n",
        "  X_all_tensor = torch.tensor(X_all_scaled, dtype=torch.float32)\n",
        "  y_pred_scaled = predict_model(net, X_all_tensor, device)\n",
        "\n",
        "  y_pred_actual = sc_y.inverse_transform(y_pred_scaled)\n",
        "  y_actual = tmp[target_name].values\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.scatter(y_actual, y_pred_actual, alpha=0.3, label=\"Data Points\")\n",
        "  plt.plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'r--', label=\"Perfect Match (1:1 Line)\")\n",
        "  plt.xlabel(f\"Actual {target_name}\")\n",
        "  plt.ylabel(f\"Predicted {target_name}\")\n",
        "  plt.title(\"Model Performance\")\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(mydir_own_data, \"own_data_kfold_model_performance.png\"))\n",
        "  plt.clf()\n"
      ],
      "metadata": {
        "id": "RMPCFwpf0bI-",
        "outputId": "abb7794b-b3c9-48d0-ed36-4e1865f01608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}